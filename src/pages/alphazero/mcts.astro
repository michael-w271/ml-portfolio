---
import Layout from '../../layouts/Layout.astro';
const base = import.meta.env.BASE_URL;
---

<Layout title="AlphaZero -- MCTS Deep Dive" description="Detailed analysis of MCTS in AlphaZero: PUCT formula, node structure, backpropagation with perspective flipping, Dirichlet noise, temperature, and scaling laws.">
  <div style="max-width: 1100px; margin: 0 auto; padding: 3rem 1.5rem;">

    <div style="margin-bottom: 2.5rem;">
      <div style="display: flex; gap: 0.5rem; flex-wrap: wrap; margin-bottom: 0.8rem;">
        <a href={`${base}alphazero/`} style="color: var(--text-secondary); text-decoration: none; font-size: 0.9rem;">AlphaZero</a>
        <span style="color: var(--text-secondary);">/</span>
        <span style="color: var(--text-primary); font-size: 0.9rem;">MCTS Deep Dive</span>
      </div>
      <div style="display: flex; gap: 0.5rem; flex-wrap: wrap; margin-bottom: 1rem;">
        <span class="tag tag-blue">MCTS</span>
        <span class="tag tag-green">PUCT</span>
        <span class="tag tag-purple">Tree Search</span>
      </div>
      <h1 class="gradient-text" style="font-size: 2.6rem; font-weight: 800; margin-bottom: 1rem;">
        MCTS Deep Dive
      </h1>
      <p style="color: var(--text-secondary); font-size: 1.15rem; max-width: 750px; line-height: 1.7;">
        The four MCTS phases in AlphaZero with code, the PUCT formula, backpropagation with
        perspective flipping, and why more simulations always means stronger play.
      </p>
    </div>

    <!-- ===================== SECTION 1: FOUR PHASES ===================== -->
    <section style="margin-bottom: 3rem;">
      <h2 style="font-size: 1.9rem; font-weight: 700; color: var(--text-primary); margin-bottom: 1.2rem; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border);">
        1. The Four MCTS Phases
      </h2>

      <pre style="font-size: 0.82rem; line-height: 1.7; margin-bottom: 1.5rem;"><code>
  +----------------------------------------------------------------------+
  |  PHASE 1: SELECTION                                                  |
  |                                                                      |
  |  Start at root. Follow child with highest PUCT score:               |
  |                                                                      |
  |    PUCT(s,a) = Q(s,a) + C * P(s,a) * sqrtN(s) / (1 + N(s,a))         |
  |                                                                      |
  |  Keep descending until a leaf node (not fully expanded).            |
  +----------------------------------------------------------------------+
                   ?
  +----------------------------------------------------------------------+
  |  PHASE 2: EXPANSION                                                  |
  |                                                                      |
  |  Call neural network: policy, value = net(state)                    |
  |  Create child nodes for all legal actions.                          |
  |  Set child.prior = policy[action] for each child.                  |
  +----------------------------------------------------------------------+
                   ?
  +----------------------------------------------------------------------+
  |  PHASE 3: EVALUATION                                                 |
  |                                                                      |
  |  value = net.value_head(state)   &lt;- no rollout needed!               |
  |  value  in  [-1, +1] (from current player's perspective)              |
  +----------------------------------------------------------------------+
                   ?
  +----------------------------------------------------------------------+
  |  PHASE 4: BACKPROPAGATION                                            |
  |                                                                      |
  |  Walk back up the path to root.                                     |
  |  For each node on the path:                                         |
  |    node.visit_count += 1                                            |
  |    node.value_sum  += value                                         |
  |    value = -value    &lt;- flip perspective for opponent's nodes        |
  +----------------------------------------------------------------------+
</code></pre>

      <pre><code class="language-python">class Node:
    def __init__(self, game, args, state, parent=None, action=None, prior=0):
        self.game         = game
        self.args         = args
        self.state        = state          # board state (copy)
        self.parent       = parent
        self.action       = action         # action that led to this node
        self.prior        = prior          # P(s,a) from policy net
        self.children     = []
        self.visit_count  = 0
        self.value_sum    = 0.0

    @property
    def is_fully_expanded(self):
        return len(self.children) > 0

    def select(self):
        """Select child with highest PUCT score."""
        return max(self.children, key=lambda c: c.puct())

    def puct(self):
        """PUCT score for this node (from parent's perspective)."""
        q = self.get_q_value()
        u = (self.args['C']
             * self.prior
             * math.sqrt(self.parent.visit_count)
             / (1 + self.visit_count))
        return q + u

    def get_q_value(self):
        """Q(s,a) -- normalised win probability in [0,1]."""
        if self.visit_count == 0:
            return 0.0
        # value_sum is in [-1, +1] from current player's perspective
        # Convert to [0, 1]: 0=loss, 0.5=draw, 1=win
        return 1 - ((self.value_sum / self.visit_count) + 1) / 2

    def expand(self, policy):
        """Create child nodes with prior probabilities from policy net."""
        for action, prob in enumerate(policy):
            if prob &gt; 0:   # only legal actions
                child_state = self.game.get_next_state(
                    self.state, action, player=1
                )
                child_state = self.game.change_perspective(
                    child_state, player=-1  # flip to opponent's view
                )
                child = Node(self.game, self.args, child_state,
                             parent=self, action=action, prior=prob)
                self.children.append(child)

    def backpropagate(self, value):
        """Update this node and all ancestors."""
        self.value_sum   += value
        self.visit_count += 1
        if self.parent is not None:
            self.parent.backpropagate(-value)  # flip sign for opponent
</code></pre>
    </section>

    <!-- ===================== SECTION 2: PUCT FORMULA ===================== -->
    <section style="margin-bottom: 3rem;">
      <h2 style="font-size: 1.9rem; font-weight: 700; color: var(--text-primary); margin-bottom: 1.2rem; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border);">
        2. The PUCT Formula
      </h2>

      <div class="formula-box" style="margin: 1rem 0 1.5rem;">
        <div style="text-align: center; font-size: 1.1rem; margin-bottom: 0.8rem;">PUCT(s, a)</div>
        PUCT(s, a) = Q(s,a) + C . P(s,a) . sqrtN(s) / (1 + N(s,a))
        <div style="margin-top: 1rem; display: grid; grid-template-columns: auto 1fr; gap: 0.3rem 1rem; font-size: 0.9rem;">
          <span style="color: var(--accent-cyan); font-weight: 700;">Q(s,a)</span>
          <span>Exploitation: mean value of this child's subtree, normalised to [0,1]</span>
          <span style="color: var(--accent-cyan); font-weight: 700;">C = 2</span>
          <span>Exploration constant -- higher = more exploration</span>
          <span style="color: var(--accent-cyan); font-weight: 700;">P(s,a)</span>
          <span>Prior probability from policy head -- intelligent exploration guidance</span>
          <span style="color: var(--accent-cyan); font-weight: 700;">sqrtN(s)</span>
          <span>Parent visit count (square root) -- exploration bonus grows with depth</span>
          <span style="color: var(--accent-cyan); font-weight: 700;">1 + N(s,a)</span>
          <span>Child visit count (+1 prevents /0); exploration decreases as child is visited more</span>
        </div>
      </div>

      <div class="prose">
        <p>
          The Q-value normalization is critical. Raw Q-values range from -1 to +1 (win/draw/loss),
          but the PUCT formula expects them in [0,1]. The conversion is:
        </p>
      </div>

      <div class="formula-box">
        Q_normalised(s,a) = 1 - (Q_raw(s,a) + 1) / 2
        <div style="margin-top: 0.5rem; font-size: 0.88rem; color: var(--text-secondary);">
          where Q_raw = value_sum / visit_count  in  [-1, +1]
          <br />
          Q_norm=0 means expected win (good),  Q_norm=1 means expected loss (bad)
        </div>
      </div>

      <div class="callout callout-info" style="margin-top: 1.2rem;">
        <strong>Note on sign convention:</strong> Board states are always stored from the current
        player's perspective. A value of +1 means "current player wins". After applying an action,
        the board is flipped so the opponent sees themselves as the current player. This means
        backpropagation must negate the value at each parent -- a +1 for the child (opponent won)
        is a -1 for the parent (current player lost).
      </div>
    </section>

    <!-- ===================== SECTION 3: NODE STRUCTURE ===================== -->
    <section style="margin-bottom: 3rem;">
      <h2 style="font-size: 1.9rem; font-weight: 700; color: var(--text-primary); margin-bottom: 1.2rem; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border);">
        3. Node Structure
      </h2>
      <div class="prose">
        <p>
          Each node in the MCTS tree represents a game state and stores:
        </p>
      </div>

      <div style="display: grid; grid-template-columns: repeat(auto-fill, minmax(240px, 1fr)); gap: 1rem; margin: 1.2rem 0;">
        <div class="stat-card" style="border-left: 3px solid var(--accent-blue);">
          <code style="color: var(--accent-blue);">state</code>
          <p style="color: var(--text-secondary); font-size: 0.88rem; margin-top: 0.5rem;">
            Board state as a numpy array, always from current player's perspective.
            Shape: (rows, cols).
          </p>
        </div>
        <div class="stat-card" style="border-left: 3px solid var(--accent-green);">
          <code style="color: var(--accent-green);">visit_count (N)</code>
          <p style="color: var(--text-secondary); font-size: 0.88rem; margin-top: 0.5rem;">
            How many times this node has been visited during MCTS simulations.
            Higher = more reliable Q estimate.
          </p>
        </div>
        <div class="stat-card" style="border-left: 3px solid var(--accent-orange);">
          <code style="color: var(--accent-orange);">value_sum (W)</code>
          <p style="color: var(--text-secondary); font-size: 0.88rem; margin-top: 0.5rem;">
            Sum of backed-up values. Q = value_sum / visit_count.
            Can be negative (opponent has been winning from this position).
          </p>
        </div>
        <div class="stat-card" style="border-left: 3px solid var(--accent-purple);">
          <code style="color: var(--accent-purple);">prior (P)</code>
          <p style="color: var(--text-secondary); font-size: 0.88rem; margin-top: 0.5rem;">
            Prior probability from policy network. Set at expansion time.
            Higher prior = more initial exploration of this action.
          </p>
        </div>
        <div class="stat-card" style="border-left: 3px solid var(--accent-cyan);">
          <code style="color: var(--accent-cyan);">children</code>
          <p style="color: var(--text-secondary); font-size: 0.88rem; margin-top: 0.5rem;">
            List of child Node objects. Empty if not yet expanded.
            One child per legal action.
          </p>
        </div>
        <div class="stat-card" style="border-left: 3px solid var(--accent-blue);">
          <code style="color: var(--accent-blue);">parent, action</code>
          <p style="color: var(--text-secondary); font-size: 0.88rem; margin-top: 0.5rem;">
            Reference to parent node and the action taken to reach this node.
            Used during backpropagation.
          </p>
        </div>
      </div>
    </section>

    <!-- ===================== SECTION 4: SEARCH LOOP ===================== -->
    <section style="margin-bottom: 3rem;">
      <h2 style="font-size: 1.9rem; font-weight: 700; color: var(--text-primary); margin-bottom: 1.2rem; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border);">
        4. Full MCTS Search Loop
      </h2>

      <pre><code class="language-python">class MCTS:
    def __init__(self, game, args, model):
        self.game  = game
        self.args  = args   # C, num_searches, dirichlet_alpha, dirichlet_epsilon
        self.model = model

    @torch.no_grad()
    def search(self, state):
        """Run MCTS from given state, return visit count distribution."""
        root = Node(self.game, self.args, state, visit_count=1)

        # Get initial policy from network and expand root
        policy, _ = self.model(
            torch.tensor(self.game.get_encoded_state(state),
                         dtype=torch.float32).unsqueeze(0)
        )
        policy = torch.softmax(policy, dim=1).squeeze(0).cpu().numpy()

        # Mask illegal actions
        valid_moves = self.game.get_valid_moves(state)
        policy     *= valid_moves
        policy     /= np.sum(policy)

        # Add Dirichlet noise at root for exploration (self-play only)
        if self.args.get('dirichlet_epsilon', 0) &gt; 0:
            alpha   = self.args['dirichlet_alpha']
            epsilon = self.args['dirichlet_epsilon']
            noise   = np.random.dirichlet([alpha] * self.game.action_size)
            policy  = ((1 - epsilon) * policy + epsilon * noise)

        root.expand(policy)

        # Run num_searches simulations
        for _ in range(self.args['num_searches']):
            node = root

            # Phase 1: Selection -- traverse to leaf
            while node.is_fully_expanded:
                node = node.select()

            # Check if terminal
            value, is_terminal = self.game.get_value_and_terminated(
                node.state, node.action
            )

            if not is_terminal:
                # Phase 2+3: Expansion + Evaluation via neural net
                policy, value = self.model(
                    torch.tensor(
                        self.game.get_encoded_state(node.state),
                        dtype=torch.float32
                    ).unsqueeze(0)
                )
                policy = torch.softmax(policy, dim=1).squeeze(0).cpu().numpy()
                valid  = self.game.get_valid_moves(node.state)
                policy = policy * valid
                if policy.sum() &gt; 0:
                    policy /= policy.sum()
                value  = value.item()
                node.expand(policy)
            else:
                value = self.game.get_opponent_value(value)

            # Phase 4: Backpropagation
            node.backpropagate(value)

        # Build action probability distribution from visit counts
        action_probs = np.zeros(self.game.action_size)
        for child in root.children:
            action_probs[child.action] = child.visit_count
        action_probs /= action_probs.sum()
        return action_probs
</code></pre>
    </section>

    <!-- ===================== SECTION 5: BACKPROPAGATION DETAIL ===================== -->
    <section style="margin-bottom: 3rem;">
      <h2 style="font-size: 1.9rem; font-weight: 700; color: var(--text-primary); margin-bottom: 1.2rem; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border);">
        5. Backpropagation with Perspective Flipping
      </h2>
      <div class="prose">
        <p>
          Perspective flipping is a subtle but essential detail. In AlphaZero, the board is always
          stored from the current player's perspective (player=1 is always "us"). After each move,
          the board is flipped so the opponent sees themselves as player 1.
        </p>
        <p>
          This means when we evaluate a leaf node and get value V (e.g., V=+1 meaning "current
          player wins"), we must negate V at each parent during backpropagation, because the parent
          is the current player's <em>opponent</em>.
        </p>
      </div>

      <pre><code>
  Root: Player 1 to move (their turn)
    |
    ?  Player 1 plays action 3
  Node A: Player 2 to move (board flipped -- P2 sees themselves as current player)
    |
    ?  Player 2 plays action 2
  Node B: Player 1 to move (board flipped again)
    |
    ?  Neural net evaluation: value = +0.7 (current player = P1 is winning)

  Backpropagation:
    Node B: value_sum += +0.7,  visit_count += 1   (good for P1)
    Node A: value_sum += -0.7,  visit_count += 1   (bad for P2, i.e. good for P1)
    Root:   value_sum += +0.7,  visit_count += 1   (good for P1)

  --- get_opponent_value(v): simply returns -v -------------------------------
</code></pre>
    </section>

    <!-- ===================== SECTION 6: DIRICHLET NOISE ===================== -->
    <section style="margin-bottom: 3rem;">
      <h2 style="font-size: 1.9rem; font-weight: 700; color: var(--text-primary); margin-bottom: 1.2rem; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border);">
        6. Dirichlet Noise at the Root
      </h2>

      <div class="formula-box">
        P~(s_root, a) = (1 - epsilon) . P(s_root, a) + epsilon . ?_a
        <div style="margin-top: 0.8rem; font-size: 0.9rem;">
          ? ~ Dirichlet(alpha=0.3),  epsilon = 0.25
        </div>
      </div>

      <div class="prose" style="margin-top: 1.2rem;">
        <p>
          Dirichlet noise is added only at the root node during self-play (not during evaluation).
          It ensures every legal action gets some minimum exploration probability, preventing the
          MCTS from always following the same line when the policy is very confident.
        </p>
        <p>
          The alpha=0.3 parameter controls the shape of the Dirichlet distribution:
        </p>
        <ul>
          <li>alpha &lt; 1 (sparse): noise concentrates on a few random moves (suitable for games with many actions)</li>
          <li>alpha = 1 (uniform): noise is uniform across all actions</li>
          <li>alpha &gt; 1 (dense): noise is spread evenly (not useful here)</li>
        </ul>
        <p>
          With alpha=0.3 and 7 actions in Connect Four, the Dirichlet noise typically gives 60-70%
          of its mass to 2-3 random actions. Combined with epsilon=0.25, about 25% of the prior
          comes from this noise, ensuring the other 4-5 actions are also occasionally explored.
        </p>
      </div>
    </section>

    <!-- ===================== SECTION 7: TEMPERATURE ===================== -->
    <section style="margin-bottom: 3rem;">
      <h2 style="font-size: 1.9rem; font-weight: 700; color: var(--text-primary); margin-bottom: 1.2rem; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border);">
        7. Temperature Parameter
      </h2>

      <div class="formula-box">
        pi(a) prop N(s,a)^{'{'}1/tau{'}'}
        <div style="margin-top: 0.6rem; font-size: 0.9rem;">
          tau -> 0: deterministic (argmax of visit counts)<br />
          tau = 1: proportional to visit counts (stochastic, high entropy)
        </div>
      </div>

      <div class="prose" style="margin-top: 1.2rem;">
        <p>
          During <strong>self-play</strong>, tau=1 is used for the first few moves (high exploration,
          diverse training data). After move 10 or so (configurable), tau drops to near 0 (greedy).
        </p>
        <p>
          During <strong>evaluation</strong>, tau=0 always (deterministic, strongest play).
        </p>
        <p>
          This temperature schedule serves a crucial purpose: early-game moves with tau=1 generate
          diverse training positions (the agent sees many different openings), while late-game
          moves with tau->0 ensure the agent finishes games optimally, producing accurate outcome
          labels.
        </p>
      </div>

      <pre><code class="language-python">def get_action(mcts_probs, temperature):
    """Sample action from MCTS visit count distribution."""
    if temperature == 0:
        # Greedy: pick most-visited action
        return np.argmax(mcts_probs)
    else:
        # Temperature scaling: pi(a) prop N^{'{'}1/tau{'}'}
        probs = mcts_probs ** (1 / temperature)
        probs /= probs.sum()
        return np.random.choice(len(probs), p=probs)
</code></pre>
    </section>

    <!-- ===================== SECTION 8: SCALING ===================== -->
    <section style="margin-bottom: 3rem;">
      <h2 style="font-size: 1.9rem; font-weight: 700; color: var(--text-primary); margin-bottom: 1.2rem; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border);">
        8. Why More Simulations = Stronger Play
      </h2>
      <div class="prose">
        <p>
          One of the most important properties of MCTS is its scaling law: more simulations per
          move always increases play strength, with diminishing returns.
        </p>
      </div>

      <div style="overflow-x: auto; margin: 1.2rem 0;">
        <table style="width: 100%; border-collapse: collapse; font-size: 0.9rem;">
          <thead>
            <tr style="background: var(--bg-secondary);">
              <th style="padding: 0.7rem 1rem; border: 1px solid var(--border);">Simulations</th>
              <th style="padding: 0.7rem 1rem; border: 1px solid var(--border);">Tree depth (approx)</th>
              <th style="padding: 0.7rem 1rem; border: 1px solid var(--border);">Win rate vs 50-sim</th>
              <th style="padding: 0.7rem 1rem; border: 1px solid var(--border);">ms per move</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">50 (training)</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~3-4</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">50% (baseline)</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~15 ms</td>
            </tr>
            <tr style="background: var(--bg-secondary);">
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">100</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~5-6</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~57%</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~30 ms</td>
            </tr>
            <tr>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">400 (evaluation)</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~8-10</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~71%</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~120 ms</td>
            </tr>
            <tr style="background: var(--bg-secondary);">
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">1,600</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~12-15</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~79%</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~480 ms</td>
            </tr>
          </tbody>
        </table>
      </div>

      <div class="prose">
        <p>
          Why does this happen? More simulations allow the tree to explore deeper, catching threats
          and opportunities that shallow search misses. With 50 simulations on Connect Four (depth ~3),
          the agent might miss a 4-move winning combination. With 400 simulations (depth ~9), it
          catches threats 9 moves ahead.
        </p>
        <p>
          This scaling property is why AlphaZero's strength is described in "compute units" rather
          than fixed capabilities -- more hardware = stronger play, without any retraining.
        </p>
      </div>

      <div class="callout callout-success">
        <strong>Training vs Evaluation simulation count:</strong> We use 50 simulations during
        training (fast games = more data) and 400 during evaluation (accurate assessment of strength).
        The training policy is "weaker" MCTS, but this diversity is actually beneficial -- it
        generates diverse training states that prevent overfitting to a single playstyle.
      </div>
    </section>

    <!-- Navigation -->
    <div style="display: flex; gap: 1rem; flex-wrap: wrap; margin-top: 2.5rem; padding-top: 1.5rem; border-top: 1px solid var(--border);">
      <a href={`${base}alphazero/`}
         style="padding: 0.7rem 1.4rem; background: var(--bg-secondary); color: var(--text-primary); border: 1px solid var(--border); border-radius: 6px; text-decoration: none; font-weight: 600;">
        &lt;- Overview
      </a>
      <a href={`${base}alphazero/network/`}
         style="padding: 0.7rem 1.4rem; background: var(--accent-purple); color: white; border-radius: 6px; text-decoration: none; font-weight: 600;">
        Neural Network ->
      </a>
      <a href={`${base}background/mcts/`}
         style="padding: 0.7rem 1.4rem; background: var(--bg-secondary); color: var(--text-primary); border: 1px solid var(--border); border-radius: 6px; text-decoration: none; font-weight: 600;">
        MCTS Background
      </a>
    </div>

  </div>
</Layout>
