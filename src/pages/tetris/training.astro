---
import Layout from '../../layouts/Layout.astro';
const base = import.meta.env.BASE_URL;
---

<Layout title="Tetris DQN -- Training Pipeline" description="Full Tetris DQN training pipeline: experience replay, target network, epsilon-greedy exploration, hyperparameters, convergence analysis, and the C++/LibTorch advantage.">
  <div style="max-width: 1100px; margin: 0 auto; padding: 3rem 1.5rem;">

    <div style="margin-bottom: 2.5rem;">
      <div style="display: flex; gap: 0.5rem; flex-wrap: wrap; margin-bottom: 0.8rem;">
        <a href={`${base}tetris/`} style="color: var(--text-secondary); text-decoration: none; font-size: 0.9rem;">Tetris DQN</a>
        <span style="color: var(--text-secondary);">/</span>
        <span style="color: var(--text-primary); font-size: 0.9rem;">Training</span>
      </div>
      <div style="display: flex; gap: 0.5rem; flex-wrap: wrap; margin-bottom: 1rem;">
        <span class="tag tag-green">Training</span>
        <span class="tag tag-blue">Experience Replay</span>
        <span class="tag tag-orange">C++/LibTorch</span>
      </div>
      <h1 class="gradient-text" style="font-size: 2.6rem; font-weight: 800; margin-bottom: 1rem;">
        Training Pipeline
      </h1>
      <p style="color: var(--text-secondary); font-size: 1.15rem; max-width: 750px; line-height: 1.7;">
        Experience replay, target network synchronisation, epsilon-greedy decay, afterstate
        batching, and the full convergence story -- from random play to 22K+ lines.
      </p>
    </div>

    <!-- ===================== FULL TRAINING LOOP ===================== -->
    <section style="margin-bottom: 3rem;">
      <h2 style="font-size: 1.9rem; font-weight: 700; color: var(--text-primary); margin-bottom: 1.2rem; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border);">
        1. The Full Training Loop
      </h2>

      <pre style="font-size: 0.82rem; line-height: 1.6;"><code>INITIALIZE:
  online_net   &lt;- QNet(40, 128)        weights from Xavier init
  target_net   &lt;- QNet(40, 128)        copy of online_net (frozen)
  replay_buf   &lt;- CircularBuffer(100K)
  optimizer    &lt;- Adam(lr=1e-4)
  epsilon      &lt;- 1.0
  step         &lt;- 0

FOR episode = 1 to MAX_EPISODES:
  state = env.reset()
  done  = False
  total_reward = 0

  WHILE not done:
    -- AFTERSTATE EVALUATION --------------------------------------
    placements = env.getLegalPlacements()         // up to 40
    features   = [env.extractFeatures(env.simulate(p))
                  for p in placements]
    batch      = torch.tensor(features)           // [N, 40]
    q_vals     = online_net.forward(batch)        // [N, 1]

    -- EPSILON-GREEDY ACTION SELECTION ----------------------------
    IF rand() < epsilon:
        action = random.choice(placements)        // explore
    ELSE:
        action = placements[argmax(q_vals)]       // exploit

    -- ENVIRONMENT STEP -------------------------------------------
    next_state, reward, done = env.step(action)

    -- STORE TRANSITION -------------------------------------------
    afterstate_features = env.extractFeatures(next_state)
    replay_buf.push(features[action_idx],
                    reward,
                    afterstate_features,
                    done)

    -- TRAIN (if enough samples) ----------------------------------
    IF len(replay_buf) >= BATCH_SIZE:
        batch = replay_buf.sample(BATCH_SIZE)
        loss  = compute_loss(batch, online_net, target_net)
        optimizer.zero_grad()
        loss.backward()
        clip_grad_norm_(online_net, max_norm=10.0)
        optimizer.step()

    -- DECAY EPSILON ----------------------------------------------
    epsilon = max(EPSILON_MIN,
                  1.0 - step * (1.0 - EPSILON_MIN) / EPSILON_DECAY)

    -- SYNC TARGET NETWORK ----------------------------------------
    IF step % TARGET_SYNC == 0:
        target_net.load_state_dict(online_net.state_dict())

    step += 1
    state = next_state

  -- LOG METRICS ------------------------------------------------
  metrics.log(episode, lines, reward, loss, epsilon, q_mean)
</code></pre>
    </section>

    <!-- ===================== EXPERIENCE REPLAY ===================== -->
    <section style="margin-bottom: 3rem;">
      <h2 style="font-size: 1.9rem; font-weight: 700; color: var(--text-primary); margin-bottom: 1.2rem; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border);">
        2. Experience Replay
      </h2>
      <div class="prose">
        <p>
          Experience replay stores past transitions in a fixed-size buffer and samples random
          mini-batches for training. Without it, the agent would train only on the most recent
          experience -- causing several problems:
        </p>
        <ul>
          <li><strong>Temporal correlations:</strong> Consecutive game states are highly correlated (similar board positions). Training on correlated data violates the i.i.d. assumption of SGD, causing oscillation and divergence.</li>
          <li><strong>Catastrophic forgetting:</strong> A policy that has learned to survive 500 moves might "forget" this when it enters a new phase of training focused on line-clearing.</li>
          <li><strong>Sample inefficiency:</strong> Each transition is used exactly once without replay. With replay, important transitions are trained on multiple times, improving data efficiency by 10-100x.</li>
        </ul>
      </div>

      <pre><code class="language-cpp">// Replay buffer in C++ -- circular deque implementation
class ReplayBuffer &#123;
    struct Transition &#123;
        std::array&lt;float, 40&gt; state;        // afterstate features
        float                  reward;
        std::array&lt;float, 40&gt; next_state;
        bool                   done;
    &#125;;

    std::deque&lt;Transition&gt; buffer_;
    size_t capacity_;

public:
    explicit ReplayBuffer(size_t capacity = 100'000)
        : capacity_(capacity) &#123;&#125;

    void push(const Transition& t) &#123;
        if (buffer_.size() >= capacity_)
            buffer_.pop_front();   // oldest transition evicted
        buffer_.push_back(t);
    &#125;

    std::vector&lt;Transition&gt; sample(size_t n) &#123;
        std::vector&lt;size_t&gt; indices(buffer_.size());
        std::iota(indices.begin(), indices.end(), 0);
        // Fisher-Yates shuffle for unbiased sampling
        std::shuffle(indices.begin(), indices.end(), rng_);
        std::vector&lt;Transition&gt; batch;
        for (size_t i = 0; i &lt; n; ++i)
            batch.push_back(buffer_[indices[i]]);
        return batch;
    &#125;

    size_t size() const &#123; return buffer_.size(); &#125;
&#125;;
</code></pre>

      <div class="callout callout-info" style="margin-top: 1rem;">
        <strong>Buffer capacity trade-off:</strong> A larger buffer (1M+) holds more diverse
        experiences but may include transitions from a very old (worse) policy, slowing convergence.
        100K is a good balance: diverse enough to break correlations, recent enough to be on-policy.
      </div>
    </section>

    <!-- ===================== TARGET NETWORK ===================== -->
    <section style="margin-bottom: 3rem;">
      <h2 style="font-size: 1.9rem; font-weight: 700; color: var(--text-primary); margin-bottom: 1.2rem; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border);">
        3. Target Network
      </h2>
      <div class="prose">
        <p>
          The Q-learning target is:
        </p>
      </div>
      <div class="formula-box">
        y = r + gamma . max_{'{'}a'{'}'} Q(s', a'; theta?)
      </div>
      <div class="prose">
        <p>
          Without a separate target network (theta?), both the prediction Q(s,a;theta) and the target y
          use the same parameters theta. This means <em>every gradient step simultaneously changes both
          the prediction and the target</em> -- like trying to hit a moving goalpost. The result
          is oscillation, divergence, or catastrophically poor convergence.
        </p>
        <p>
          The target network theta? is a copy of the online network theta that is <strong>frozen</strong>
          for TARGET_SYNC=2000 steps, then hard-synced:
        </p>
      </div>

      <pre><code class="language-cpp">// Periodic hard sync of target network
if (step % TARGET_SYNC_INTERVAL == 0) &#123;
    // Copy all parameters from online to target
    target_net->load_state_dict(online_net->state_dict());
    target_net->eval();   // disable dropout, etc.
    // Detach target params from computation graph
    for (auto& p : target_net->parameters())
        p.set_requires_grad(false);
&#125;
</code></pre>

      <div class="prose">
        <p>
          The sync frequency of 2000 steps is a balance: too frequent (e.g., 100) and the target
          moves too fast; too infrequent (e.g., 50,000) and the target is too stale.
        </p>
      </div>

      <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.2rem; margin-top: 1rem;">
        <div class="stat-card" style="border-left: 3px solid var(--accent-orange);">
          <h4 style="color: var(--accent-orange); margin-bottom: 0.5rem;">Without Target Network</h4>
          <p style="color: var(--text-secondary); font-size: 0.9rem;">
            Q-values oscillate. Training may diverge. Loss bounces up and down even when
            the policy is improving. Common symptom: Q-values grow without bound.
          </p>
        </div>
        <div class="stat-card" style="border-left: 3px solid var(--accent-green);">
          <h4 style="color: var(--accent-green); margin-bottom: 0.5rem;">With Target Network</h4>
          <p style="color: var(--text-secondary); font-size: 0.9rem;">
            Stable training. Loss decreases monotonically (mostly). Q-values converge to
            meaningful values. Each 2000-step "epoch" has a fixed regression target.
          </p>
        </div>
      </div>
    </section>

    <!-- ===================== EPSILON-GREEDY ===================== -->
    <section style="margin-bottom: 3rem;">
      <h2 style="font-size: 1.9rem; font-weight: 700; color: var(--text-primary); margin-bottom: 1.2rem; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border);">
        4. Epsilon-Greedy Exploration
      </h2>
      <div class="prose">
        <p>
          The exploration schedule decays linearly from epsilon=1.0 (fully random) to epsilon=0.01 (nearly
          greedy) over 200,000 steps:
        </p>
      </div>

      <div class="formula-box">
        epsilon(t) = max( epsilon_min,  1.0 - t x (1.0 - epsilon_min) / T_decay )
        <div style="margin-top: 0.8rem; font-size: 0.88rem; color: var(--text-secondary);">
          epsilon_min = 0.01,  T_decay = 200,000 steps
        </div>
      </div>

      <div class="prose">
        <p>
          The exploration phases have different functions:
        </p>
        <ul>
          <li><strong>epsilon=1.0 (steps 0-10K):</strong> Fully random play. Fills the replay buffer with diverse, unbiased transitions. The agent discovers what happens in different board states.</li>
          <li><strong>epsilon=0.5 (steps ~50K):</strong> Balanced exploration. The policy is starting to be meaningful but still explores widely.</li>
          <li><strong>epsilon=0.1 (steps ~150K):</strong> Mostly exploitation with occasional exploration. The policy is strong enough that random actions are mostly harmful -- but occasional exploration prevents overfitting to one strategy.</li>
          <li><strong>epsilon=0.01 (steps 200K+):</strong> Nearly greedy. The agent exploits its learned policy with minimal perturbation.</li>
        </ul>
      </div>

      <pre><code>
  epsilon  1.0 | ??
         | ????
         | ??????
     0.5 |         ????????
         |                 ????????
     0.1 |                         ??????????????
    0.01 |                                       ????????????????
         +------------------------------------------------------> steps
                 50K          100K          150K         200K+
</code></pre>
    </section>

    <!-- ===================== HYPERPARAMETERS ===================== -->
    <section style="margin-bottom: 3rem;">
      <h2 style="font-size: 1.9rem; font-weight: 700; color: var(--text-primary); margin-bottom: 1.2rem; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border);">
        5. Hyperparameters
      </h2>
      <div class="prose">
        <p>
          All hyperparameters are loaded from <code>training_params.json</code> at startup,
          allowing sweeps without recompiling. Here are the final values:
        </p>
      </div>

      <div style="overflow-x: auto; margin-top: 1.2rem;">
        <table style="width: 100%; border-collapse: collapse; font-size: 0.9rem;">
          <thead>
            <tr style="background: var(--bg-secondary);">
              <th style="padding: 0.7rem 1rem; border: 1px solid var(--border); text-align: left;">Parameter</th>
              <th style="padding: 0.7rem 1rem; border: 1px solid var(--border); text-align: left;">Value</th>
              <th style="padding: 0.7rem 1rem; border: 1px solid var(--border); text-align: left;">Description</th>
            </tr>
          </thead>
          <tbody>
            <tr><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">learning_rate</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-blue);">0.0001</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Adam optimizer learning rate</td></tr>
            <tr style="background:var(--bg-secondary)"><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">gamma</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-blue);">0.99</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Discount factor</td></tr>
            <tr><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">batch_size</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-blue);">512</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Mini-batch size for each gradient update</td></tr>
            <tr style="background:var(--bg-secondary)"><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">replay_capacity</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-blue);">100,000</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Maximum transitions in replay buffer</td></tr>
            <tr><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">target_sync</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-blue);">2,000</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Steps between target network hard sync</td></tr>
            <tr style="background:var(--bg-secondary)"><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">epsilon_start</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-blue);">1.0</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Initial exploration rate</td></tr>
            <tr><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">epsilon_end</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-blue);">0.01</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Minimum exploration rate</td></tr>
            <tr style="background:var(--bg-secondary)"><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">epsilon_decay</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-blue);">200,000</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Steps to decay from epsilon_start to epsilon_end</td></tr>
            <tr><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">hidden_size</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-blue);">128</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Hidden layer width (both layers)</td></tr>
            <tr style="background:var(--bg-secondary)"><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">grad_clip</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-blue);">10.0</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Max gradient norm (gradient clipping)</td></tr>
            <tr><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">warm_up_steps</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-blue);">10,000</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Steps of pure random play before training begins</td></tr>
            <tr style="background:var(--bg-secondary)"><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">save_interval</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-blue);">10,000</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Episodes between model checkpoint saves</td></tr>
            <tr><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">reward_line[1-4]</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-green);">10, 30, 60, 100</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Tiered line-clear rewards</td></tr>
            <tr style="background:var(--bg-secondary)"><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">penalty_holes</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-orange);">-4.0</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Per-new-hole penalty multiplier</td></tr>
            <tr><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">penalty_bumpiness</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-orange);">-0.1</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Per-unit bumpiness penalty</td></tr>
            <tr style="background:var(--bg-secondary)"><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">penalty_height</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-orange);">-0.1</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Per-unit aggregate height increase penalty</td></tr>
            <tr><td style="padding:0.6rem 1rem;border:1px solid var(--border);font-family:monospace;">penalty_game_over</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);color:var(--accent-orange);">-100.0</td><td style="padding:0.6rem 1rem;border:1px solid var(--border);">Terminal game-over penalty</td></tr>
          </tbody>
        </table>
      </div>
    </section>

    <!-- ===================== TRAINING METRICS ===================== -->
    <section style="margin-bottom: 3rem;">
      <h2 style="font-size: 1.9rem; font-weight: 700; color: var(--text-primary); margin-bottom: 1.2rem; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border);">
        6. Training Metrics to Watch
      </h2>

      <div style="display: grid; grid-template-columns: repeat(auto-fill, minmax(260px, 1fr)); gap: 1.2rem; margin-bottom: 1.5rem;">
        <div class="stat-card" style="border-left: 3px solid var(--accent-blue);">
          <h4 style="color: var(--accent-blue); margin-bottom: 0.5rem;">Huber Loss</h4>
          <p style="color: var(--text-secondary); font-size: 0.88rem;">
            Should decrease over training. Large spikes after target network sync are normal.
            A plateau usually indicates the learning rate needs adjustment or the capacity is maxed.
          </p>
        </div>
        <div class="stat-card" style="border-left: 3px solid var(--accent-green);">
          <h4 style="color: var(--accent-green); margin-bottom: 0.5rem;">Lines per Episode</h4>
          <p style="color: var(--text-secondary); font-size: 0.88rem;">
            The primary performance metric. Should increase monotonically on average.
            High variance is normal; track 100-episode rolling average.
          </p>
        </div>
        <div class="stat-card" style="border-left: 3px solid var(--accent-orange);">
          <h4 style="color: var(--accent-orange); margin-bottom: 0.5rem;">Q-value Magnitude</h4>
          <p style="color: var(--text-secondary); font-size: 0.88rem;">
            If Q-values grow without bound (Q-value explosion), reduce learning rate or increase
            gradient clipping. Healthy values: 0 to ~200 for this reward function.
          </p>
        </div>
        <div class="stat-card" style="border-left: 3px solid var(--accent-purple);">
          <h4 style="color: var(--accent-purple); margin-bottom: 0.5rem;">Epsilon</h4>
          <p style="color: var(--text-secondary); font-size: 0.88rem;">
            Monotonically decreasing. Once epsilon &lt; 0.1, performance should accelerate as the agent
            exploits its learned policy.
          </p>
        </div>
      </div>
    </section>

    <!-- ===================== C++ ADVANTAGE ===================== -->
    <section style="margin-bottom: 3rem;">
      <h2 style="font-size: 1.9rem; font-weight: 700; color: var(--text-primary); margin-bottom: 1.2rem; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border);">
        7. The C++ Advantage
      </h2>
      <div class="prose">
        <p>
          Training this agent in C++ with LibTorch rather than Python + PyTorch provides
          meaningful performance benefits:
        </p>
      </div>

      <div style="overflow-x: auto; margin: 1.2rem 0;">
        <table style="width: 100%; border-collapse: collapse; font-size: 0.9rem;">
          <thead>
            <tr style="background: var(--bg-secondary);">
              <th style="padding: 0.7rem 1rem; border: 1px solid var(--border);">Component</th>
              <th style="padding: 0.7rem 1rem; border: 1px solid var(--border);">Python + PyTorch</th>
              <th style="padding: 0.7rem 1rem; border: 1px solid var(--border);">C++ + LibTorch</th>
              <th style="padding: 0.7rem 1rem; border: 1px solid var(--border);">Speedup</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">Tetris engine (sim 40 placements)</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~180 uss</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~8 uss</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border); color: var(--accent-green);">22x</td>
            </tr>
            <tr style="background: var(--bg-secondary);">
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">Feature extraction (40-dim)</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~25 uss</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~1.5 uss</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border); color: var(--accent-green);">17x</td>
            </tr>
            <tr>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">Network forward pass (batch=40)</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~0.4 ms</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~0.35 ms</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border); color: var(--accent-blue);">1.1x</td>
            </tr>
            <tr style="background: var(--bg-secondary);">
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">Replay buffer sample + train</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~2.1 ms</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~1.8 ms</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border); color: var(--accent-blue);">1.2x</td>
            </tr>
            <tr>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">Overall throughput (steps/sec)</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border);">~1,200 steps/s</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border); font-weight: 600;">~8,400 steps/s</td>
              <td style="padding: 0.6rem 1rem; border: 1px solid var(--border); color: var(--accent-green); font-weight: 700;">7x</td>
            </tr>
          </tbody>
        </table>
      </div>

      <div class="prose">
        <p>
          The GPU is primarily used for the batched forward pass. The game simulation and feature
          extraction are CPU-bound, and C++ is dramatically faster here because it avoids Python's
          GIL, interpreter overhead, and numpy array creation costs.
        </p>
        <p>
          The build system uses CMake with automatic CUDA detection. The LibTorch package provides
          pre-built CUDA-enabled shared libraries; the CMakeLists.txt links against them directly.
        </p>
      </div>

      <pre><code class="language-cmake"># CMakeLists.txt (excerpt)
cmake_minimum_required(VERSION 3.18)
project(TetrisDQN)

set(CMAKE_CXX_STANDARD 17)

# Find LibTorch -- set TORCH_DIR to your LibTorch install
find_package(Torch REQUIRED)
find_package(SDL2 REQUIRED)

add_executable(train
    src/main.cpp
    src/tetris_env.cpp
    src/dqn_agent.cpp
    src/replay_buffer.cpp
    src/feature_extractor.cpp
)

target_link_libraries(train
    PRIVATE
    "$&#123;TORCH_LIBRARIES&#125;"
    SDL2::SDL2
)

# Enable CUDA if available
if(CUDA_FOUND)
    target_compile_definitions(train PRIVATE USE_CUDA)
    message(STATUS "CUDA detected -- using GPU training")
endif()
</code></pre>
    </section>

    <!-- ===================== CONVERGENCE ===================== -->
    <section style="margin-bottom: 3rem;">
      <h2 style="font-size: 1.9rem; font-weight: 700; color: var(--text-primary); margin-bottom: 1.2rem; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border);">
        8. Convergence Analysis
      </h2>
      <div class="prose">
        <p>
          The agent goes through several distinct phases of learning. Here is the story of
          training from episode 0 to 500K:
        </p>
      </div>

      <pre><code>
Episode Range     Avg Lines   Epsilon   Q-value Mean   What's Happening
-------------------------------------------------------------------------
0 - 10K           12          1.0       0.0            Pure random play, filling buffer
10K - 50K         35          0.80      -15.2          Learning to avoid immediate holes
50K - 100K        180         0.55      -8.4           Learns basic survival, reduces holes
100K - 150K       890         0.30      +2.1           Discovers line-clearing strategies
150K - 200K       2,400       0.10      +18.5          Policy solidifies; exploitation phase
200K - 300K       4,100       0.01      +31.2          Fine-tuning in exploitation regime
300K - 500K       7,800       0.01      +45.7          Continued improvement via replay
500K+             8,300       0.01      +47.1          Near-convergence (plateau)
</code></pre>

      <div class="prose">
        <p>
          Three critical transition points:
        </p>
        <ul>
          <li><strong>~10K steps:</strong> Training begins (warm-up complete). The very first gradient signal.</li>
          <li><strong>~100K steps:</strong> The agent "gets it" -- Q-values become positive as it learns that line-clearing is good. Average lines jumps from ~180 to ~900 over 50K steps.</li>
          <li><strong>~200K steps:</strong> Epsilon reaches 0.01. The agent can now fully exploit its policy. Performance doubles from ~2,400 to ~4,100 lines as exploration overhead is removed.</li>
        </ul>
        <p>
          The all-time best of 22,645 lines occurs as a lucky run during the fully-converged
          phase (500K+), where the agent gets a favorable piece sequence and plays near-perfectly
          for thousands of moves.
        </p>
      </div>

      <div class="callout callout-success">
        <strong>Total compute:</strong> 500,000 episodes x ~40 steps/episode = 20M environment steps.
        At 8,400 steps/second: approximately 40 minutes of training on an RTX 4090 class GPU.
        The limiting factor is replay buffer sampling and the CPU-side game simulation, not the GPU.
      </div>
    </section>

    <!-- Navigation -->
    <div style="display: flex; gap: 1rem; flex-wrap: wrap; margin-top: 2.5rem; padding-top: 1.5rem; border-top: 1px solid var(--border);">
      <a href={`${base}tetris/reward-shaping/`}
         style="padding: 0.7rem 1.4rem; background: var(--bg-secondary); color: var(--text-primary); border: 1px solid var(--border); border-radius: 6px; text-decoration: none; font-weight: 600;">
        &lt;- Reward Shaping
      </a>
      <a href={`${base}alphazero/`}
         style="padding: 0.7rem 1.4rem; background: var(--accent-green); color: white; border-radius: 6px; text-decoration: none; font-weight: 600;">
        AlphaZero Project ->
      </a>
      <a href={`${base}compare/`}
         style="padding: 0.7rem 1.4rem; background: var(--bg-secondary); color: var(--text-primary); border: 1px solid var(--border); border-radius: 6px; text-decoration: none; font-weight: 600;">
        DQN vs AlphaZero
      </a>
    </div>

  </div>
</Layout>
